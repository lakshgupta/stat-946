{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook discusses the use of recurrent neural networks for sequence modelling tasks and structured prediction problems. A recurrent network, to explain, has feedback connections that allow that state of a layer in the network at one time step to affect the state of the same layer at the next time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = nltk.load('tokenizers/punkt/english.pickle')\n",
    "stemmer = SnowballStemmer('english')\n",
    "min_len = 10\n",
    "\n",
    "with open('data/sample.txt','r') as f:\n",
    "    text = f.read().split('</doc>')    \n",
    "\n",
    "# Remove url info and title from each document\n",
    "docs = [re.sub(\"<.*>\", \"\", doc) for doc in text]\n",
    "# docs = [doc.split('\\n')[3:] for doc in docs]\n",
    "docs = [doc.split('\\n') for doc in docs]\n",
    "docs = [' '.join(doc) for doc in docs]\n",
    "\n",
    "# Remove unicode from each document \n",
    "docs = [doc.decode('unicode_escape') for doc in docs]\n",
    "docs = [doc.encode('ascii','ignore') for doc in docs]\n",
    "docs = [tokenizer.tokenize(doc) for doc in docs]\n",
    "\n",
    "# Join tokenized documents into a list of sentences\n",
    "sen_list = [sen for doc in docs for sen in doc]\n",
    "sen_list = [s.translate(None, string.punctuation) for s in sen_list]\n",
    "sen_list = [s.translate(None, '1234567890') for s in sen_list]\n",
    "sen_list = [nltk.word_tokenize(s.lower()) for s in sen_list]\n",
    "sen_list = [s for s in sen_list if len(s) > min_len]\n",
    "sen_list = [s+['.'] for s in sen_list]\n",
    "\n",
    "# Build vocab from sentence list\n",
    "def flatten(lst, acc):\n",
    "    for item in lst:\n",
    "        if type(item) == type([]):\n",
    "            flatten(item, acc)\n",
    "        else:\n",
    "            acc.append(item)\n",
    "    return acc\n",
    "\n",
    "words = flatten(sen_list, [])\n",
    "\n",
    "counts = collections.Counter()\n",
    "counts.update(words)\n",
    "\n",
    "vocab = sorted([x for x,y in counts.iteritems()])\n",
    "vocab.append('UNK')\n",
    "\n",
    "wrd_to_ind = {j:i for i,j in enumerate(vocab)}\n",
    "ind_to_wrd = {i:j for i,j in enumerate(vocab)}\n",
    "\n",
    "data = ['UNK' if w not in vocab else w for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequences(seqlen=10, iters=10):\n",
    "    count = 0\n",
    "    for i in range(iters):\n",
    "        for _ in range(len(data)): \n",
    "            if len(data) - _ > 10:\n",
    "                x_words = data[_:_+seqlen]\n",
    "                y_words = data[_+1:_+seqlen+1]    \n",
    "                yield x_words, y_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    def __init__(self, vocab, dim, eps=0.05):\n",
    "        \n",
    "        # Randomly initialize the three weight matrices\n",
    "        self.U = np.random.random((dim, len(vocab)))*eps*2-eps\n",
    "        self.W = np.random.random((dim, dim))*eps*2-eps\n",
    "        self.V = np.random.random((len(vocab), dim))*eps*2-eps\n",
    "        self.xs, self.hs, self.ys = {}, {}, {}\n",
    "        self.bh = np.zeros(dim)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def get_onehot(self, ind):\n",
    "        onehot = np.zeros(len(self.vocab))\n",
    "        onehot[ind] = 1\n",
    "        return onehot\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        \n",
    "    def get_activities(self, seq_in):\n",
    "        self.hs[-1] = np.zeros(len(self.W))\n",
    "        for t in range(len(seq_in)):\n",
    "            self.xs[t] = self.get_onehot(seq_in[t])\n",
    "            self.hs[t] = np.tanh(np.dot(self.U, self.xs[t])+np.dot(self.W, self.hs[t-1])+self.bh)\n",
    "            self.ys[t] = self.softmax(np.dot(self.V, self.hs[t]))\n",
    "    \n",
    "    def train(self, rate=0.05):\n",
    "        for i,o in sequences():\n",
    "            xs = np.array([wrd_to_ind[wrd] for wrd in i])\n",
    "            ts = np.array([wrd_to_ind[wrd] for wrd in o])\n",
    "            \n",
    "            self.get_activities(xs)\n",
    "            \n",
    "            U_grad = np.zeros_like(self.U)\n",
    "            W_grad = np.zeros_like(self.W)\n",
    "            V_grad = np.zeros_like(self.V)\n",
    "            bh_grad = np.zeros_like(self.bh)\n",
    "            \n",
    "            h_grads = {}\n",
    "            h_grads[len(ts)] = np.zeros(len(self.W))\n",
    "\n",
    "            for _ in reversed(range(len(ts))):\n",
    "                y_grad = self.ys[_] - self.get_onehot(ts[_])\n",
    "                h_grads[_] = (np.dot(self.V.T, y_grad)+np.dot(self.W.T, h_grads[_+1]))\n",
    "                h_grads[_] = h_grads[_] * (1 - self.hs[_]**2)\n",
    "\n",
    "                U_grad += np.outer(h_grads[_], self.xs[_])\n",
    "                W_grad += np.outer(h_grads[_+1], self.hs[_])\n",
    "                V_grad += np.outer(y_grad, self.hs[_])\n",
    "                bh_grad += h_grads[_]\n",
    "                    \n",
    "            grads = [U_grad, W_grad, V_grad, bh_grad]\n",
    "            \n",
    "            # Clip gradients to avoid explosions\n",
    "            for _ in range(len(grads)):\n",
    "                if np.linalg.norm(grads[_]) > 5:\n",
    "                    grads[_] = 5 * grads[_] / np.linalg.norm(grads[_]) \n",
    "\n",
    "            U_grad = grads[0]\n",
    "            W_grad = grads[1]\n",
    "            V_grad = grads[2]\n",
    "            bh_grad = grads[3]\n",
    "                \n",
    "            self.U += -rate * U_grad\n",
    "            self.W += -rate * W_grad\n",
    "            self.V += -rate * V_grad \n",
    "            self.bh += -rate * bh_grad\n",
    "                \n",
    "    def predict(self, start, steps=5):\n",
    "        hs, ys = {}, {}\n",
    "        output = []\n",
    "        word = start\n",
    "        hs[-1] = np.zeros(len(self.W))\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            ind = wrd_to_ind[word]\n",
    "            hs[_] = np.tanh(np.dot(self.U, self.get_onehot(ind))+np.dot(self.W, hs[_-1])+self.bh)\n",
    "            ys[_] = self.softmax(np.dot(self.V, hs[_]))\n",
    "            next_word = ind_to_wrd[np.argmax(ys[_])]\n",
    "            output.append(next_word)\n",
    "            word = next_word\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = RNN(vocab, dim=1500)\n",
    "test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "['as', 'an', 'antidogmatic', 'philosophy', 'anarchism']\n",
      "\n",
      "a\n",
      "['political', 'philosophy', 'that', 'advocates', 'stateless']\n",
      "\n",
      "advocates\n",
      "['stateless', 'societies', 'often', 'defined', 'as']\n",
      "\n",
      "an\n",
      "['antidogmatic', 'philosophy', 'anarchism', 'draws', 'on']\n",
      "\n",
      "anarchism\n",
      "['entails', 'opposing', 'authority', 'or', 'hierarchical']\n",
      "\n",
      "and\n",
      "['philosophy', 'anarchism', 'draws', 'on', 'many']\n",
      "\n",
      "antidogmatic\n",
      "['philosophy', 'anarchism', 'draws', 'on', 'many']\n",
      "\n",
      "antistatism\n",
      "['is', 'central', 'anarchism', 'entails', 'opposing']\n",
      "\n",
      "as\n",
      "['an', 'antidogmatic', 'philosophy', 'anarchism', 'draws']\n",
      "\n",
      "associations\n",
      "['.', 'while', 'antistatism', 'is', 'central']\n",
      "\n",
      "authority\n",
      "['or', 'hierarchical', 'organisation', 'in', 'the']\n",
      "\n",
      "authors\n",
      "['have', 'defined', 'as', 'more', 'specific']\n",
      "\n",
      "based\n",
      "['on', 'nonhierarchical', 'free', 'associations', '.']\n",
      "\n",
      "but\n",
      "['not', 'limited', 'to', 'the', 'state']\n",
      "\n",
      "central\n",
      "['anarchism', 'entails', 'opposing', 'authority', 'or']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in vocab[:15]:\n",
    "    print word\n",
    "    print test.predict(word, steps=5)\n",
    "    print ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
