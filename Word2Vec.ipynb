{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Word2Vec is a recently developed family of algorithms for generating distributed representations of words from a large corpus of text. These distributed representations are sometimes referred to as 'word embeddings' because the algorithms that generate them are designed to embed a vocabulary of words in a relatively low dimensional vector space (i.e. the dimensionality of the embeddings is usually on the order of hundreds, while the size of the vocabulary is usually on the order of tens or hundreds of thousands). Well known methods for creating word embeddings include LSA, HAL, BEAGLE, GloVe, and Word2Vec. Word2Vec's popularity as an embedding model is due to the fact that the representations it produces are able capture interesting relationships involving multiple words. \n",
    "\n",
    "Interestingly, Word2Vec uses two distinct model architectures: the CBOW (continuous-bag-of-words) architecture and the Skip-gram architecture. To explain, the CBOW architecture learns to predict a target word given a 'bag' or context of surrounding words. The Skip-gram architecture, on the other hand, does exactly the opposite - it learns to predict a bag of surrounding words given a target word. The figure below, taken from Mikolov et al. (2013), provides a nice visual depiction of each architecture, where $w$ indicates a word, and $t$ indicates the target word position.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/cbow.png\" width=500px></center>  \n",
    "<br>\n",
    "\n",
    "The rest of this notebook describes each model in some mathematical detail, drawing on explanations provided by Rong (2015). The notebook also provides example applications of each algorithm to a corpus of approximately 5,000 wikipedia articles. Optimizations concerning the use of hierarchical softmax and negative sampling are also discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Continuous Bag of Words (CBOW)\n",
    "\n",
    "As illustrated in the figure above, the CBOW architecture is akin to a three layer neural network. The input layer encodes a binary vector indicating a bag of words (order is not important, so the $w(t-1)...$ notation in the figure merely indicates the window around the target word from which the bag is drawn), and the hidden layer maps the input layer into a low dimensional space without applying a non-linearity. Finally, the hidden layer is mapped to a softmax output layer that gives a probability distribution over the words in the model's vocabulary. \n",
    "\n",
    "In the simplest case, the input is a single word drawn from a corpus, and the target is the next word in the corpus. The input is encoded as a \"one-hot\" vector, and hence it extracts a single column from the input-to-hidden weight matrix ($W_1$). As such the value of the hidden layer is $h = W_1 x$, where $x$ is the input vector. Next, each unit in the output layer encodes a dot product between $h$ and a row in the hidden-to-output weight matrix ($W_2$). The softmax function is then used to convert these dot products into a proper probability distribution. It is important to highlight that each column of $W_1$ is vector representation of a particular word, as is each row of $W_2$. (So each word has *two* vector representations). The goal of learning is to make the $W_1$ vector representation of a word most similar to the $W_2$ vector representation of the word that is most likely to follow it in the corpus. \n",
    "\n",
    "Alternatively, one can think of the model as performing a variant of logistic regression using a factored weight matrix. The hidden layer does not include an element-wise non-linearity, so a weight matrix between the input and output layers is factored into two $N x V$ and $V x N$ components, where $V$ is the size of the vocabulary, and $N$ is the dimensionality of the hidden layer. These component matrices each contain a distributed representation of each word in the vocabulary - call them $v_w$ and $v_{w}^{'}$ for word $w$. So, the activation of the output layer after applying the softmax is as follows for input word $i$ and output word $o$:\n",
    "\n",
    "$y_o = \\frac{e^{v_i^{T} v_o^{'}}}{\\sum_{j=1}^{V} e^{v_i^{T} v_j^{'}}}$\n",
    "\n",
    "Learning is conducted by applying gradient descent to minimize a cost function defined in terms of the negative log-likelihood of the correct output word $o$. This cost function is similar to the one used previously for multiclass logistic regression (see the notebook on backpropogation):\n",
    "\n",
    "$J(\\theta) = -log(\\frac{e^{v_i^{T} v_o^{'}}}{\\sum_{j=1}^{V} e^{v_i^{T} v_j^{'}}}) = - v_i^{T} v_o^{'} + log\\sum_{j=1}^{V} e^{v_i^{T} v_j^{'}}$  \n",
    "\n",
    "where again $j$ is the index of the correct output word, $i$ is the index of the input word, and $V$ is the size of the vocabulary. Differentiating this cost function with respect to the total input to each output unit gives the familiar softmax derivative:\n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial v_i^{T} v_j^{'}} = \\sum \\limits_j \\frac{\\partial J(\\theta)}{\\partial y_j} \\frac{\\partial y_j}{\\partial v_i^{T} v_j^{'}} =  y_j - t_j $\n",
    "\n",
    "where $t_k$ is the target output for unit $k$. With this derivative, we can compute the gradient of the weights in the network using backprogation. At an intuitive level, the updates to $W_2$ are easy to understand. Each row $j$ in $W_2$ is updated with values of the hidden layer activities, but scaled by both the learning rate and the prediction error at $y_k$ (i.e. the softmax derivative). So, for a given output unit, if the model incorrectly guesses that the word corresponding to this unit ought to be predicted, the unit's incoming weights will be decremented to be more *dissimilar* to the hidden layer vector. In effect, this lowers the value of the dot product between the first layer representation of the input word, and the second layer representations of the output word under consideration. On the other hand, if the model incorrectly fails to predict the correct word, the unit corresponding to this word will have its incoming weights incremented to become more *similar* to the hidden layer vector. This raises the value of the dot product between the first layer representation of the input word and the second layer representation of the correct output word. As such, when same input is provided again to the model, it will be more likely to predict the correct output. The updates to $W_1$ follow a similar but less directly interpretable pattern. \n",
    "\n",
    "With a large-sized vocabulary, CBOW is actually quite slow to train, mostly due to the computation of the softmax function on the output layer. We can see this by training a model on a few thousand documents from wikipedia. Training is performed by attempting to predict each word in each sentence of every document from a bag of up to six surrounding words (generalizing to multiple input words does not change any of the math - the hidden layer just ends up encoding a sum of the first layer representations of each input word). \n",
    "\n",
    "To give a demonstration, the first thing to do is to build a vocabulary by getting word counts from a collection of documents. Words with low counts will be ignored to avoid idiosyncratic mispellings and extremely rare words; the remaining words will form the vocabulary. \n",
    "\n",
    "Counting words can be sped up using python's multiprocessing library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Words:  15738309\n",
      "Vocabulary Size:  27263\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import multiprocessing\n",
    "from utils import docstream, countwords\n",
    "\n",
    "counts = collections.Counter()  \n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Stream n files from wikipedia dump and count words in parallel\n",
    "for dlist in docstream(size=100):\n",
    "    results = pool.map_async(countwords, dlist)\n",
    "    for r in results.get():\n",
    "        counts.update(r)\n",
    " \n",
    "vocab = [x[0] for x in counts.iteritems() if x[1] > 25]\n",
    "print 'Total Number of Words: ', sum(counts.values())\n",
    "print 'Vocabulary Size: ', len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the CBOW model using a variation on the neural network class used in other notebooks. Some helper functions for grabbing contexts from sentences and converting words into one-hot vectors have been defined elsewhere to keep things manageably concise. We'll get access to these functions by inheriting from an EmbeddingModel class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import embedding # Functions for get contexts from sentences etc.\n",
    "\n",
    "class CBOW(embedding.EmbeddingModel):\n",
    "    \"\"\"\n",
    "    A shallow neural network that implements the 'continous bag of words' \n",
    "    learning algorithm described in Mikolov et al (2013). \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    voc : dict\n",
    "        The vocabulary of words that embeddings are being learned for.\n",
    "    dim : int \n",
    "        The dimensionality of the hidden layer of the network.\n",
    "    eps : float, optional\n",
    "        Scaling factor on random weight initialization. By default, the \n",
    "        weightsare chosen from a uniform distribution on the interval \n",
    "        [-0.1, 0.1].\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, dim, eps=0.15):\n",
    "        self.w1 = np.random.random((dim, len(vocab)))*eps*2-eps\n",
    "        self.w2 = np.random.random((len(vocab), dim))*eps*2-eps\n",
    "        self.word_indices = {j:i for i,j in enumerate(vocab)}\n",
    "        self.win_size = 3\n",
    "        self.vocab = vocab\n",
    "            \n",
    "    def get_activations(self, x):\n",
    "        self.yh = np.dot(self.w1, x.T)\n",
    "        self.yo = self.softmax(np.dot(self.w2, self.yh))\n",
    "\n",
    "    def train(self, ndocs, rate=0.3): \n",
    "        for xs, ys in self.batch_generator(ndocs):\n",
    "            bsize = float(xs.shape[1])\n",
    "            # Compute activations\n",
    "            self.get_activations(xs.T)\n",
    "\n",
    "            # Compute gradients               \n",
    "            yo_grad = self.yo-ys\n",
    "            yh_grad = np.dot(self.w2.T, yo_grad)\n",
    "\n",
    "            w1_grad = np.dot(yh_grad, xs.T) / bsize\n",
    "            w2_grad = np.dot(yo_grad, self.yh.T) / bsize\n",
    "\n",
    "            # Update weights\n",
    "            self.w1 += -rate * w1_grad\n",
    "            self.w2 += -rate * w2_grad\n",
    "        \n",
    "        # Create word vectors from network weights\n",
    "        self.word_vecs = self.w1.T + self.w2\n",
    "        norms = np.linalg.norm(self.word_vecs, axis=1)\n",
    "        self.word_vecs = np.divide(self.word_vecs, norms[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedder = CBOW(vocab, dim=150) \n",
    "embedder.train(ndocs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors to \"small\":\n",
      "\n",
      "small 1.0\n",
      "large 0.692444277791\n",
      "tiny 0.660107399535\n",
      "larger 0.653754282846\n",
      "smaller 0.583412575441\n",
      "\n",
      "Nearest neighbors to \"roman\":\n",
      "\n",
      "roman 1.0\n",
      "catholic 0.673838975686\n",
      "byzantine 0.643910891448\n",
      "catholicism 0.60339234428\n",
      "empire 0.60302562371\n",
      "\n",
      "Nearest neighbors to \"medieval\":\n",
      "\n",
      "medieval 1.0\n",
      "renaissance 0.701334013228\n",
      "modern 0.604067449688\n",
      "iconography 0.598731651187\n",
      "hellenistic 0.594439571047\n",
      "\n",
      "Nearest neighbors to \"cash\":\n",
      "\n",
      "cash 1.0\n",
      "payments 0.657618525796\n",
      "payment 0.655440486\n",
      "banks 0.622166553811\n",
      "loans 0.612372415847\n",
      "\n",
      "Nearest neighbors to \"car\":\n",
      "\n",
      "car 1.0\n",
      "accident 0.68987610779\n",
      "crash 0.608334304968\n",
      "injured 0.60722705516\n",
      "drunk 0.597927025079\n",
      "\n",
      "Nearest neighbors to \"waves\":\n",
      "\n",
      "waves 1.0\n",
      "wave 0.617193281391\n",
      "propagate 0.607981741084\n",
      "seismic 0.568776460093\n",
      "light 0.531950285785\n",
      "\n",
      "Nearest neighbors to \"french\":\n",
      "\n",
      "french 1.0\n",
      "italian 0.70883228134\n",
      "english 0.707912943792\n",
      "german 0.689125854016\n",
      "norwegian 0.634899206725\n",
      "\n",
      "Nearest neighbors to \"english\":\n",
      "\n",
      "english 1.0\n",
      "german 0.711347023886\n",
      "french 0.707912943792\n",
      "translated 0.670404909966\n",
      "translation 0.660690656286\n",
      "\n",
      "Nearest neighbors to \"texas\":\n",
      "\n",
      "texas 1.0\n",
      "oklahoma 0.673511002185\n",
      "rangers 0.63255035105\n",
      "houston 0.632199528816\n",
      "austin 0.614888602516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_list = ['small','roman','medieval','cash','car','waves','french','english','texas']\n",
    "for query in query_list:\n",
    "    print 'Nearest neighbors to \"%s\":' %query\n",
    "    embedder.get_synonyms(query)\n",
    "    print ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Skip-gram\n",
    "\n",
    "Content in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
